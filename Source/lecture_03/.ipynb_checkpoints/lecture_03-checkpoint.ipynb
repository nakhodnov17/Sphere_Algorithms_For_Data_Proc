{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/header.png\">\n",
    "\n",
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "\n",
    "## Лекция 3. K-means и EM алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Сегодня на лекции\n",
    "\n",
    "* Смесь нормальных распределений и EM\n",
    "* K-means и его модификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## На предыдущей лекции\n",
    "\n",
    "**Дано.** Признаковые описания $N$ объектов $\\mathbf{x} = (x_1, \\ldots, x_m) \\in \\mathcal{X}$, образующие тренировочный набор данных $X$  \n",
    "\n",
    "**Найти.** Модель из семейства параметрических функций \n",
    "\n",
    "$$H = \\{h(\\mathbf{x, \\mathbf{\\theta}}): \\mathcal{X} \\times \\Theta \\rightarrow \\mathcal{Y} \\mid \\mathcal{Y} = \\{1, \\ldots, K\\}\\},$$\n",
    "ставящую в соответствие произвольному $\\mathbf{x} \\in \\mathcal{X}$ один из $K$ кластеров так, чтобы объекты внутри одного кластера были похожи, а объекты из разных кластеров различались  \n",
    "\n",
    "**Алгоритмы.** Hierarchical Clustering, dbscan, OPTICS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Нужно отметиться на лекции\n",
    "\n",
    "https://sphere.mail.ru/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Смесь нормальных распределений и EM\n",
    "<img src=\"images/local.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Гаусс, Карл Фридрих (1777-1855)\n",
    "\n",
    "<img src=\"images/gauss.jpg\">\n",
    "\n",
    "* Не открыл распределение Гаусса\n",
    "* Открыл все остальное"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Иога́нн Карл Фри́дрих Га́усс (нем. Johann Carl Friedrich Gauß; 30 апреля 1777, Брауншвейг — 23 февраля 1855, Гёттинген) — немецкий математик, механик, физик, астроном и геодезист[12]. Считается одним из величайших математиков всех времён, «королём математиков»[13]\n",
    "\n",
    "https://ru.wikipedia.org/wiki/%D0%93%D0%B0%D1%83%D1%81%D1%81,_%D0%9A%D0%B0%D1%80%D0%BB_%D0%A4%D1%80%D0%B8%D0%B4%D1%80%D0%B8%D1%85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Начнем с простого (смоделируем один кластер)\n",
    "\n",
    "**Данные**   \n",
    "Координаты точек попаданий по мишени из гауссовской пушки  \n",
    "\n",
    "**Задача**  \n",
    "Определить, куда смещен прицел  \n",
    "\n",
    "<img src=\"images/target.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Многомерное нормальное распределение\n",
    "\n",
    "$$\\mathcal{N(\\mathbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}}) = \\frac{1}{(2 \\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\exp \\left\\{-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T \\mathbf{\\Sigma^{-1}} (\\mathbf{x} - \\mathbf{\\mu})\\right\\}$$\n",
    "\n",
    "**Параметры**  \n",
    "\n",
    "\n",
    "${D}$-мерный вектор средних\n",
    "\n",
    "$$\\mathbf{\\mu} = \\int \\mathbf{x} p({\\mathbf{x}}) d\\mathbf{x}$$\n",
    "\n",
    "$D \\times D$-мерная матрица ковариации   \n",
    "\n",
    "\n",
    "$$\\mathbf{\\Sigma} = E[(\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^T]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Матрица ковариации симметрична\n",
    "\n",
    "$\\mathbf{x}$ - размерности $D$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$D = 2$$\n",
    "<img src=\"images/multi.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/gnormal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\mathbf{\\Sigma} = \\text{diag}(\\sigma_i)$$\n",
    "<img src=\"images/dnormal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\mathbf{\\Sigma} = \\sigma I$$\n",
    "<img src=\"images/enormal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Чтобы выяснить параметры распределения модели необходимо вычислить параметры:  \n",
    "$\\mu$: $D$ параметров\n",
    "$\\Sigma$: $(D+1)*D/2$, так как матрица симметричная, считается из арифметической прогрессии.\n",
    "\n",
    "Итого: $(D+3)*D/2$ параметров \n",
    "\n",
    "Это довольно много, поэтому делаются предположения о матрице ковариации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Формализуем задачу\n",
    "\n",
    "Имеется набор данных\n",
    "$$X = \\{\\mathbf{x}_n \\in R^2\\}$$\n",
    "\n",
    "Предположение\n",
    "$$p(\\mathbf{x}_n) \\sim \\mathcal{N(\\mathbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}}), \\quad \\mu \\in R^2, \\;\\; \\mathbf{\\Sigma} \\in R^{2 \\times 2}$$\n",
    "\n",
    "Требуется найти вектор средних $\\mu$ и матрицу ковариации $\\mathbf{\\Sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Maximum likelihood\n",
    "\n",
    "Принцип максимального правдоподобия  \n",
    "\n",
    "Пусть дано семейство параметрических моделей $h(\\mathbf{x}, \\mathbf{\\theta})$. Выбираем вектор параметров $\\theta$, максимизирующий функцию правдоподобия (likelihood) $p(\\mathcal{D} | \\theta)$, соответствующую рассматриваемому семейству моделей.\n",
    "\n",
    "Правдоподобие  \n",
    "$$L(X | \\mu, \\mathbf{\\Sigma}) = \\prod_{n=1}^N \\mathcal{N}(\\mathbf{x_n} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) \\rightarrow \\max_{\\mu, \\mathbf{\\Sigma}}$$\n",
    "\n",
    "Решение\n",
    "$$\\mu_{ML} = \\frac 1 N \\sum_{n=1}^N \\mathbf{x_n}, \\quad \\mathbf{\\Sigma}_{ML} = \\frac 1 N \\sum_{n=1}^N (\\mathbf{x_n} - \\mu_{ML}) (\\mathbf{x_n} - \\mu_{ML})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Old Faithful data set\n",
    "\n",
    "$D$ = date of recordings in month (in August)  \n",
    "$X$ = duration of the current eruption in minutes   \n",
    "$Y$ = waiting time until the next eruption in minutes  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/of1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/of2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Смесь нормальных распределений\n",
    "\n",
    "*Скрытая* $K$-мерная переменная $\\mathbf{z}$ - принадлежность объекта к одному из кластеров\n",
    "\n",
    "$$p(z_k = 1) = \\pi_k, \\quad z_k \\in \\{0, 1\\}, \\quad \\sum_k z_k = 1 \\quad\\rightarrow\\quad p(\\mathbf{z}) = \\prod_k \\pi_k^{z_k}$$\n",
    "\n",
    "Распределение $\\mathbf{x}$ для каждого из $K$ кластеров\n",
    "\n",
    "$$p(\\mathbf{x} | \\mathbf{z_k}) = \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k) \\quad \\rightarrow \\quad p(\\mathbf{x} | \\mathbf{z}) = \\prod_k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)^{z_k}$$\n",
    "\n",
    "\n",
    "Смесь нормальных распределений\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_k \\pi_k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Введем $z$ - скрытая $k$-мерная переменная \n",
    "Каждый $k$-й компонент $z$ отражает принадлежит точка кластеру или нет\n",
    "$k$ - количество кластеров\n",
    "\n",
    "Введем на $z_k$ априорное распределение. Т.е. допустим, что ничего не знаем про нашу задачу, но хотим иметь хоть что-то.\n",
    "Нужно прийти к векторной форме. Вероятность $\\mathbf{z}$ в точности будет равна $\\pi_k$\n",
    "\n",
    "Каким может быть $x$ если он принадлежит $k$-му кластеру?\n",
    "Приведем это в векторную форму. Делаем такой же трюк.\n",
    "\n",
    "То, что видим на картинке это $p(x|\\pi_k, \\mu, \\Sigma)$, далее будем опускать параметры\n",
    "\n",
    "$p(x) = p(x|z)p(z)$, хотим избавиться от $z$, поэтому просуммируем по $z_k$\n",
    "\n",
    "введем еще величину $\\gamma(z_k)$ - вероятность $z_k = 1$ после того, как пронаблюдали $x$\n",
    "\n",
    "формула Байеса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Апостериорная вероятность принадлежности к $k$ кластеру (априорная равна $\\pi_k$)\n",
    "$$\n",
    "\\gamma(z_k) = p(z_k = 1 | \\mathbf{x}) = \\frac{p(z_k=1) p(\\mathbf{x} | z_k = 1)}{\\sum_{j=1}^K p(z_j=1) p(\\mathbf{x} | z_j = 1)} = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(\\mathbf{x} | \\mathbf{\\mu}_j, \\mathbf{\\Sigma}_j)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Maximum Likelihood\n",
    "Функция правдоподобия\n",
    "$$\n",
    "\\log p(\\mathbf{X} | \\mathbf{\\pi}, \\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\sum_{n=1}^N \\log \\sum_k \\pi_k \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k) \\rightarrow \\max_{\\mathbf{\\pi}, \\mathbf{\\mu}, \\mathbf{\\Sigma}}\n",
    "$$\n",
    "\n",
    "Сложности\n",
    "* схлопывание компонент\n",
    "* переименование кластеров\n",
    "* невозможно оптимизировать аналитически\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Вероятности очень малы. Переходят к логарифму. Функция непрерывно возрастающая, поэтому максимум функции и максимум логарифма функции будут совпадать.\n",
    "\n",
    "Наблюдаем N объектов.\n",
    "\n",
    "Возьмум функцию правдоподобия, продифференцируем по всем параметрам, приравняем к нулю.\n",
    "\n",
    "Схлопывание компонент - реакция на выбросы. Нужно следить за этой ситуацией. Сигма стремится к нулю    \n",
    "Переименование - дважды запустив алгоритм можем получить такой же по сути, но отличающийся по меткам результат  \n",
    "В одномерном случае оптимизировать аналитически возможно  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Есть набор моделей. Хотим выбрать ту, которая наилучшим образом сообветствует нашим данным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Дифференцируем функцию правдоподобия\n",
    "$$\n",
    "N_k = \\sum_{n=1}^N \\gamma(z_{nk}), \\;\\; \\mu_k = \\frac 1 {N_k} \\sum_{n=1}^N \\gamma(z_{nk}) \\mathbf{x}_n\n",
    "$$\n",
    "$$\n",
    "\\Sigma_k = \\frac 1 {N_k} \\sum_{n=1}^N \\gamma(z_{nk}) (\\mathbf{x}_n - \\mu_k)^T (\\mathbf{x}_n - \\mu_k)\n",
    "$$\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation Maximization\n",
    "\n",
    "**E** Expectation: при фиксированных $\\mu_k, \\Sigma_k, \\pi_k$\n",
    "$$\n",
    "\\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N} (\\mathbf{x}_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N} (\\mathbf{x}_n | \\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "**M** Maximization: при фиксированных $\\gamma(z_{nk})$\n",
    "$$\n",
    "N_k = \\sum_{n=1}^N \\gamma(z_{nk}), \\;\\; \\mu_k = \\frac 1 {N_k} \\sum_{n=1}^N \\gamma(z_{nk}) \\mathbf{x}_n\n",
    "$$\n",
    "$$\n",
    "\\Sigma_k = \\frac 1 {N_k} \\sum_{n=1}^N \\gamma(z_{nk}) (\\mathbf{x}_n - \\mu_k)(\\mathbf{x}_n - \\mu_k)^T\n",
    "$$\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\n",
    "$$\n",
    "**S** Остановиться при достижении сходимости\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/gauss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EM-алгоритм\n",
    "\n",
    "**Дано.**  \n",
    "Известно распределение $P(\\mathbf{X}, \\mathbf{Z} | \\theta)$, где $\\mathbf{x}$ - наблюдаемые переменные, а $\\mathbf{z}$ - скрытые. \n",
    "\n",
    "**Найти.**  \n",
    "$\\theta$,  максимизирующее $P(\\mathbf{X} | \\theta)$.\n",
    "\n",
    "**E** вычислить $P(\\mathbf{Z} | \\mathbf{X}, \\theta^{old})$ при фиксированном $\\theta^{old}$\n",
    "**M** вычислить $\\theta^{new} = \\arg \\max_{\\theta} \\mathcal{Q} (\\theta, \\theta^{old})$, где\n",
    "$$\n",
    "\\mathcal{Q} (\\theta, \\theta^{old}) = E_\\mathbf{Z}[\\ln p(\\mathbf{X}, \\mathbf{Z} | \\theta)] = \\sum_{\\mathbf{Z}} p(\\mathbf{Z} | \\mathbf{X}, \\theta^{old}) \\ln p(\\mathbf{X}, \\mathbf{Z} | \\theta))\n",
    "$$\n",
    "\n",
    "*Улучшение:* ввести априорное распределение $p(\\theta)$\n",
    "\n",
    "\n",
    "[Maximum Likelihood from Incomplete Data via the EM Algorithm](http://web.mit.edu/6.435/www/Dempster77.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Сходимость\n",
    "\n",
    "<img src=\"images/convergence.png\">\n",
    "\n",
    "[The Expectation Maximization Algorithm: A short tutorial](http://www.cs.cmu.edu/~dgovinda/pdf/recog/EM_algorithm-1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Различные смеси\n",
    "\n",
    "<img src=\"images/mixtures.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Перерыв\n",
    "## Тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-means и его модификации\n",
    "<img src=\"images/k-cover.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means\n",
    "\n",
    "Пусть $\\Sigma_k = \\epsilon I$, тогда\n",
    "$$\n",
    "p(\\mathbf{x} | \\mu_k, \\Sigma_k) = \\frac{1}{\\sqrt{2\\pi\\epsilon}}\\exp(-\\frac{1}{2\\epsilon}\\|\\mathbf{x}-\\mathbf{\\mu_k}\\|^2)\n",
    "$$\n",
    "Рассмотрим стремление $\\epsilon \\rightarrow 0$\n",
    "$$\n",
    "\\gamma(z_{nk}) = \\frac{\\pi_k \\exp(-\\frac{1}{2\\epsilon}\\|\\mathbf{x}_n-\\mathbf{\\mu_k}\\|^2)}{\\sum_j \\pi_j \\exp(-\\frac{1}{2\\epsilon}\\|\\mathbf{x}_n-\\mathbf{\\mu_j}\\|^2)} \\rightarrow r_{nk} = \\begin{cases}\n",
    "1, \\; \\text{для } k = \\arg \\min_j \\|\\mathbf{x}_n - \\mathbf{\\mu_j}\\|^2 \\\\\n",
    "0, \\; \\text{иначе}\n",
    "\\end{cases}\n",
    "$$\n",
    "Функция правдоподобия\n",
    "$$\n",
    "E_\\mathbf{Z}[\\ln p(\\mathbf{X}, \\mathbf{Z} | \\mu, \\Sigma, \\pi)] \\rightarrow -\\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\| \\mathbf{x}_n - \\mu_k \\|^2 + const\n",
    "$$\n",
    "Вектор средних\n",
    "$$\n",
    "\\mathbf{\\mu}_k = \\frac{\\sum_n r_{nk} \\mathbf{x_n}}{\\sum_n r_{nk}} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means\n",
    "\n",
    "```\n",
    "function kmeans(X, K):\n",
    "\tinitialize N # number of objects\n",
    "\tinitialize Mu = (mu_1 ... mu_K) # random centroids\n",
    "\tdo:\n",
    "\t\t# E step\n",
    "\t\tfor k in 1..K:\n",
    "\t\t\tfor x in 1..N:\n",
    "\t\t\t\tcompute r_nk # Cluster assignment\n",
    "\t\t# M step\n",
    "\t\tfor k in 1..K:\n",
    "\t\t\trecompute mu_k # Update centroids\n",
    "\tuntil Mu converged\n",
    "\tJ = loss(X, Mu)\n",
    "\treturn Mu, J\n",
    "```\n",
    "Сложность $O(NK)$   \n",
    "Локальная оптимизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/kmeans.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Задача\n",
    "\n",
    "<img src=\"images/task.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Модификации k-means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* На каждом шаге работаем с $b$ случайно выбранными объектами из каждого кластера (mini-batch k-means)\n",
    "<img src=\"images/mbatch.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Критерий качества (k-medoids)\n",
    "$$\n",
    "\\tilde J = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} d(\\mathbf{x}_n, \\mu_k)\n",
    "$$\n",
    "$d$ - функция расстояния, $\\mu_k$ - один из объектов в кластере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* k-means++ - *умная* инициализация. [k-means++: The Advantages of Careful Seeding](http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* онлайн k-means\n",
    "$$\\mu_k^{new} = \\mu_k^{old} + \\eta_n (\\mathbf{x}_n - \\mu_k^{old})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Кластеризация\n",
    "**Идея**  \n",
    "Выбрать критерий качества кластеризации $J$ и расстояние между объектами $d(\\mathbf{x}_i, \\mathbf{x}_j)$ и вычислить разбиение выборки на кластеры, которое соответствует оптимальному значению выбранного критерия.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Альтернативные критерии качества\n",
    "**Критерий**  \n",
    "$$\n",
    "J = \\sum_{k=1}^K \\sum_{\\mathbf{x}_i \\in C_k} \\| \\mathbf{x}_i - \\mathbf{m}_k \\|^2 = \n",
    "\\frac 1 2 \\sum_{k=1}^K n_k \\left[ \\frac{1}{n_k^2} \\sum_{\\mathbf{x}_i \\in C_k} \\sum_{\\mathbf{x}_j \\in C_k} \\| \\mathbf{x}_i - \\mathbf{x}_j \\|^2 \\right] = \n",
    "\\frac 1 2 \\sum_{k=1}^K n_k \\left[ \\frac{1}{n_k^2} \\sum_{\\mathbf{x}_i \\in C_k} \\sum_{\\mathbf{x}_j \\in C_k} s(\\mathbf{x}_i, \\mathbf{x}_j) \\right] = \\frac 1 2 \\sum_{k=1}^K n_k \\bar s_k\n",
    "$$\n",
    "\n",
    "**Примеры $\\bar s_i$**  \n",
    "$$\n",
    "\\underline{s}_k = \\min_{\\mathbf{x}_i, \\mathbf{x}_j} s(\\mathbf{x}_i, \\mathbf{x}_j); \\quad \\bar s_k = \\max_{\\mathbf{x}_i, \\mathbf{x}_j} s(\\mathbf{x}_i, \\mathbf{x}_j)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Задача на дом\n",
    "\n",
    "Рассмотреть смесь из $D$-мерных распределений Бернулли. В такой смеси $\\mathbf{x}$ -- $D$-мерный бинарный вектор, каждый компонент $x_i$ которого подчиняется распределению Бернулли с параметром $\\mu_{ki}$ при заданном векторе $\\mu_k$:\n",
    "$$\n",
    "p(\\mathbf{x} | \\mu_k) = \\prod_{i=1}^D \\mu_{ki}^{x_i} (1-\\mu_{ki})^{(1-x_i)}\n",
    "$$\n",
    "Вероятность $k$-го вектора $\\mu_k$ равна $\\pi_k$. Выписать выражения для E и M шагов при использовании EM-алгоритма для нахождения неизвестных параметров $\\mu_k$ и $\\pi_k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf  \n",
    "https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html  \n",
    "http://www.machinelearning.ru/wiki/images/1/16/S04_matrix_calculations.pdf  \n",
    "http://stats.stackexchange.com/questions/27436/how-to-take-derivative-of-multivariate-normal-density  \n",
    "http://www.math.uwaterloo.ca/~hwolkowi//matrixcookbook.pdf  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Вопросы\n",
    "### Пожалуйста, напишите отзыв о лекции"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
